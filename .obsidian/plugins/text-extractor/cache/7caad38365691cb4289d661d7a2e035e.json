{"path":".obsidian/plugins/text-extractor/cache/7caad38365691cb4289d661d7a2e035e.json","text":"Eurographics/ ACM SIGGRAPH Symposium on Computer Animation (2007) D. Metaxas and J. Popovic (Editors) Screen Space Meshes Matthias Müller Simon Schirm Stephan Duthaler AGEIA Abstract We present a simple yet powerful approach for the generation and rendering of surfaces deﬁned by the boundary of a three-dimensional point cloud. First, a depth map plus internal and external silhouettes of the surface are generated in screen space. These are used to construct a 2D screen space triangle mesh with a new technique that is derived from Marching Squares. The resulting mesh is transformed back to 3D world space for the computation of occlusions, reﬂections, refraction, and other shading effects. One of the main applications for screen space meshes is the visualization of Lagrangian, particle-based ﬂuids models. Our new method has several advantages over the full 3D Marching Cubes approach. The algorithm only generates surface where it is visible, view-dependent level of detail comes for free, and interesting visual effects are possible by ﬁltering in screen space. Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Animation and Virtual Reality 1. Introduction The Marching Cubes method [LC87] is the most popular technique for generating triangle meshes along iso-surfaces of scalar ﬁelds. One important application is the reconstruc- tion of the liquid-air interface in ﬂuid simulations. In Eule- rian ﬂuid simulations, the liquid surface is often represented as the zero level set of the scalar level set function which is advected by the velocity ﬁeld of the ﬂuid [FF01, EMF02]. The Marching Cubes technique is then used to construct a triangle mesh along the zero level set. In Lagrangian, particle-based approaches [GM77, MCG03, PTB∗03], the liquid surface is typically deﬁned as an iso-surface of a den- sity ﬁeld which is the superposition of radially symmetric kernel functions of the individual particles [Bli82]. Again, a triangle mesh is generated using the Marching Cubes algo- rithm. Although Marching Cubes is certainly the most popular and useful algorithm to generate 3D triangle meshes for iso-surfaces of scalar ﬁelds, there are some disadvantages when used for the visualization of surfaces of liquids. First, since the standard approach is camera-independent, many invisible triangles and surface details are generated. Second, the algorithm operates in three dimensions although what is sought is the front 2D surface. This surface is to be found by marching through a 3D data set. Off-line tracking of the free surface of liquids is a well understood problem with a large body of work. However, the authors are not aware of any methods for rendering the full 3D air-liquid interface that is fast enough for the use in games. Our method closes this gap. It is signiﬁcantly faster than previous methods. It does, however, only render the front most layer of the surface. This is not a limitation for opaque liquids like milk or oil and in most use cases with transparent ﬂuids, especially in connection with fake refrac- tion shaders, the artifacts are minimal as our examples show. The advantages of the new screen space mesh approach are • The screen space mesh resolves parts of the surface which are close to the camera with more triangles than distant parts, yielding camera-dependent level of detail. • Since it operates in two dimensions, a method derived from Marching Squares can be employed which is sub- stantially faster than the 3D Marching Cubes algorithm. • In contrast to other screen space approaches such as ray- tracing or point splatting, fast standard triangle shading hardware can be used for state-of-the-art forward shading of the surface and occlusion culling since the mesh can easily be transformed back into world space. Copyright c⃝ 2007 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or class- room use is granted without fee provided that copies are not made or distributed for com- mercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior speciﬁc permission and/or a fee. Request per- missions from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permis- sions@acm.org. SCA 2007, San Diego, California, August 04 - 05, 2007 c⃝ 2007 ACM 978-1-59593-624-4/07/0008 $ 5.00 M. Müller et al. / Screen Space Meshes Figure 1: Left: Final rendering. Middle: The screen space mesh. Right: Rotated view of the mesh to show its dependence on the viewing direction. • With a mesh deﬁned in screen space, the smoothing of depths and silhouettes can easily be separated and con- trolled individually. 2. Related Work Our approach is a screen space technique. Many render- ing algorithms operate primarily in screen space. The most prominent among these is raytracing [Whi80]. For each pixel, a ray is cast into the scene which is then traced along reﬂected and refracted directions yielding view-dependent level of detail. The array of pixels could be interpreted as a high-resolution regular grid in screen space which is pro- jected back into world space. However, in contrast to screen space meshes, ray tracing cannot directly make use of the tri- angle rendering pipeline of modern graphics hardware. Also, the resolution must always match the pixel resolution while the resolution of screen space meshes is adjustable to the time budget for surface rendering. The projected grid concept introduced recently by Johan- son et al. [Joh04] is more closely related to screen space meshes. It is based on earlier work on real-time ocean ren- dering [HNC02]. While the projected grid is designed to ren- der unbounded water surfaces represented by height ﬁelds, screen space meshes can be used to render arbitrary three- dimensional (ﬂuid) surfaces. Because the water surface is considered unbound in [Joh04], a projected grid is sufﬁcient. No connectivity is generated in screen space and, thus, no silhouettes can be represented. Since we present screen space meshes as a method to render the surface of point-sampled volumes, the approach is also related to point splatting methods [ZPvBG01]. The generation of the depth map is similar to a point splatting step while the rest of our algorithm deviates completely from point splatting. 3. Basic Algorithm We expect as input a set of 3D points x1, . . . xN ∈ R3 (in our case the locations of a set of particles simulated with Smoothed Particles Hydrodynamics), the projection matrix P ∈ R4×4, and a set of parameters as shown in Table 1. The basic steps of the algorithm are: 1. Setup regular depth map 2. Find internal and external silhouettes 3. Smooth depth values 4. Generate a 2D triangle mesh using our Marching Squares related procedure 5. Smooth silhouettes 6. Transform mesh back into world space 7. Render the 3D triangle mesh These steps are executed whenever the points change their locations or the camera moves. Steps 3. and 5. are explained in Section 4 as they are extensions to the basic method. Parameter Description Range h screen spacing 1 − 10 r particle size ≥ 1 nﬁlter ﬁlter size for depth smoothing 0 − 10 niters silhouette smoothing iterations 0 − 10 zmax depth connection threshold > rz Table 1: Summary of parameters. 3.1. Depth Map Setup Let W and H be the width and height of the screen in pixels. One of the input parameters of the method is the screen spac- ing h ∈ R which does not need to be an integer. The spacing deﬁnes a regular grid of cell size h with Nx = ⌈ W h ⌉ + 1 nodes horizontally and Ny = ⌈ H h ⌉ + 1 nodes vertically. The depth map Z ∈ RNx×Ny stores depth values zi, j at each node of the regular grid. It is generated from scratch at the beginning of each frame. First the depth values zi, j are initialized with ∞. Then, the algorithm iterates through all N particles twice. In the ﬁrst c⃝ Association for Computing Machinery, Inc. 2007. M. Müller et al. / Screen Space Meshes rz rp h zmax> Figure 2: Left: Side view of depth map generated by three particles. Only front most hits are considered. Large z- differences of adjacent depth values indicate inner and outer silhouettes. Right: Between adjacent nodes at most one ad- ditional node (white dot) is stored to indicate the silhouette. phase, the depth values are set. In the second phase, addi- tional depth values are generated where silhouettes cut the grid (see Fig. 2). In both phases, the coordinates and radius of each particle have to be transferred from world to screen space. This is done as follows: Let x = [x, y, z, 1]T be the homogenous coordinates of the particle considered. These coordinates are transformed using the projection matrix P to get     x′ y′ z′ w     = P     x y z 1     . (1) We assume the setup of the projection matrix to be de- ﬁned as in OpenGL and DirectX. In that case, perspective division (i.e. division of x′, y′ and z′ by w) yields canonical coordinates in the range −1 . . . 1 for all three coordinates. However, we perform perspective division only on x′ and y′ but not on the depth z′ because the depth would get dis- torted non-linearly by this transformation. We compute the projected coordinates as   xp yp zp   =   W · ( 1 2 + 1 2 x′/w) H · ( 1 2 + 1 2 y′/w) z′   . (2) This way, we have xp ∈ [0 . . .W ], yp ∈ [0 . . . H] while zp is the non-distorted distance to the camera. Another parameter of our method is the particle size r. For the projected radii we get   rx ry rz   =      rW √p2 1,1 + p2 1,2 + p2 1,3/w rH√p2 2,1 + p2 2,2 + p2 2,3/w r√p2 3,1 + p2 3,2 + p2 3,3      , (3) where the pi, j are the entries of the projection matrix P. This equation only holds at the center of the screen. For a wide ﬁeld of view the particles get distorted far from the cen- ter, an effect we ignore here since it only affects the shape, not the location of the particle. In OpenGL and DirectX√p2 3,1 + p2 3,2 + p2 3,3 = 1 so rz = r and zp is the distance to the camera. We assume that the aspect ratio of the projection is chosen according to the viewport (i.e. W /H). In this case we have a single projected radius rp = rx = ry in the screen plane and the particles appear as circles rather than ellipses. In the ﬁrst phase, for each particle, all depth values at (i, j) with (ih − xp)2 + ( jh − yp)2 ≤ r2 p are updated as zi, j ← min (zi, j, zp − rzhi, j) , (4) (5) where hi, j = √ 1 − (ih−xp)2+( jh−yp)2 r2 p . Omitting the square root in hi, j produces an upside down parabola instead of a spherical surface. This version is faster to compute and suf- ﬁcient in most cases. The reader might want to experiment with other kernels too. At the end of the ﬁrst phase, the depth map of the point cloud is coarsely sampled at the grid nodes. Similar to Marching Squares we insert additional nodes on the grid edges connecting nodes with large depth differences in order to track the silhouette in more detail as described in the next section (see also Fig. 2 right). 3.2. Silhouette Detection zmax> silhouette edge Figure 3: Left: Top view of the grid. The depth differences at the ends of bold segments are above zmax, i.e. they are cut by one or more silhouettes. Right: Side view. The lower two particles generate two different cuts on the edge. Taking the cut (white point) furthest from the end with the smaller depth value (left most in this case) removes this ambiguity. For the detection of silhouettes, the algorithm iterates through the particles a second time. In this phase, only grid edges which connect depth values that are further apart then zmax are considered (see Fig. 3). We call them silhouette edges. The goal of the algorithm is to ﬁnd one additional node (a silhouette node) on each silhouette edge. Each sil- houette node is located between the adjacent nodes of its silhouette edge and stores the depth value of the front layer. For each particle p all cuts of the circle at position (xp, yp) c⃝ Association for Computing Machinery, Inc. 2007. M. Müller et al. / Screen Space Meshes and radius rp in screen space with silhouette edges are com- puted. Each cut is a silhouette node candidate. Its location is the location of the cut and its depth is the depth zp of the particle. However, this point is only stored if two conditions hold: • The depth zp of the particle is smaller than the average depth of the silhouette edge, i.e. belongs to the front layer. This criterion can be used as an early out to avoid com- puting the cut point. • The location of the cut on the silhouette edge is further from the endpoint with the smaller depth value than a po- tential silhouette node previously stored for this edge. In that case the new node replaces the old one (see Fig. 3 right). 3.3. Mesh Generation The next step is to generate the vertices and triangles of the screen space mesh. Each grid node with an initialized depth value (̸= ∞) generates one vertex at its location with the same depth value. Each silhouette edge with only one initialized adjacent node generates one additional silhouette vertex at the location and with the depth of its silhouette node. This vertex will be on the outer silhouette. Each silhouette edge with both adjacent nodes initialized generates two vertices, both at the location of the silhouette node. One vertex asso- ciated with the end point with the smaller depth (the front vertex) gets the depth of the silhouette node. The other ver- tex (the back vertex) gets a depth value extrapolated from the neighborhood of endpoint with the larger depth value. These two nodes lie on an inner silhouette (see Fig. 4). In this ﬁg- ure linear extrapolation is used to ﬁnd the depth of the back silhouette vertex which we found to be sufﬁcient. silhouette edge silhouette edge back silhouette vertex front silhouette vertex 1 1 2 2 3 3 Figure 4: Side view of the grid. Left: Silhouette node created on a silhouette edge. Right: The mesh vertices and triangles generated for this conﬁguration. Note that two vertices with different depth values are generated for the silhouette node. To generate triangles, the mesh generation algorithm vis- its all grid cells one by one and generates triangles for them. Each of the cell’s edges is either a silhouette edge or a reg- ular edge. This leads to 16 cases as shown in Fig. 5. The number of a particular case can be computed by associating each edge with a bit and set the bit to one if the edge is cut, i.e. has a silhouette node. 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Figure 5: All the cases for the generation of a 2D triangle mesh from cut edges. In Fig. 5 we have opened the cuts on each edge for illus- tration purposes to distinguish the silhouettes vertex associ- ated with the left and the one associated with the right end node. From the triangles shown, only those are generated for which all three vertices exist. In the case of outer silhouettes only a subset of the triangles need to be added. Special care has to be taken for different groups of cases: • Case 0 represents inner triangles away from silhouettes. We alternate between the conﬁguration shown and the one which is rotated 90 degrees to get a nicer inner mesh. • Cases 3, 5, 6, 9, 10 and 12 are the regular situations with the silhouette entering at one edge and leaving at another. No special care has to be taken here. • Cases 1, 2, 4 and 8 are generated by an inner silhouette starting within the cell. The triangulation shown is not unique. To avoid ﬂickering it is important to stick with one arbitrary choice though. The difference of the depth of the two silhouette vertices is bound by 3zmax. There- fore, it is safe to connect the vertices as shown without generating arbitrarily stretched triangles. • The remaining cases 7, 11, 13, 14 and 15 are the patholog- ical ones. Have a closer look at case 7. The center triangle is connected to the left two triangles. It is, thus, connected to the same layer as those. The upper right and lower right triangles each belong to different layers which can be arbi- trarily far from the layer of the center triangle. Therefore a third vertex needs to be generated for the silhouette node on the right. Its depth needs to be extrapolated from the depths of the four vertices on the left. In case 15 two ad- ditional vertices are generated and connected to the layer of the lower left triangle. 3.4. Transformation to World Space and Rendering At this stage, we have a triangle mesh with the correct con- nectivity and vertices in screen space, e.g. with coordinates [xp, yp, zp]T . In order to render this mesh and to compute re- ﬂections and refractions of the 3D environment, the vertices are projected back into world space while the connectivity is kept ﬁxed. We, therefore, need to invert the transformation given in Eq. (1) and Eq. (2). Let Q ∈ R4×4 be the inverse of the projection matrix, i.e. Q = P−1. With Q, the world coor- c⃝ Association for Computing Machinery, Inc. 2007. M. Müller et al. / Screen Space Meshes dinates [x, y, z]T can be computed via the inverse projection equation     x y z 1     = Q     (−1 + 2xp/W )w (−1 + 2yp/H)w zp w     . (6) At this point, we do not have the projective divisor w. However, it can be retrieved (using the last row of the above equation) as w = 1 − q4,3zp q4,1(−1 + 2xp/W ) + q4,2(−1 + 2yp/H) + q4,4 (7) from known quantities only, before the inverse transforma- tion is executed. After the transformation of the mesh, we generate per- vertex normals in world space. There are several ways to compute vertex normals for a triangle mesh. We use the nor- malized sum of the normals of adjacent triangles weighted by the adjacent angles. Finally the triangles and normals are sent to the standard graphics pipeline. 4. Extensions 4.1. Depth Smoothing The procedure described so far produces bumpy depth maps. Fortunately, the depth map can easily be smoothed by apply- ing an appropriate ﬁlter. We use a separable binomial ﬁlter of user speciﬁed half-size nﬁlter (see Fig. 6), both, in i and j direction. 1/64 6/64 15/64 20/64 15/64 6/64 1/64 nFilter ii-1i-2i-3 i+1 i+2 i+3 Figure 6: Separable binomial ﬁlter with half-length nﬁlter = 3. The center depth value at (i, j) is replaced by the weighted sum of neighboring depth values. The weights used are shown inside the squares. In a ﬁrst pass, the horizontal ﬁlter is applied to all values zi, j ̸= ∞ of the depth map. In a second pass the values zi, j ̸= ∞ are ﬁltered again using the vertical version of the ﬁlter. When applying the ﬁlter, special care has to be taken near silhouettes. When the ﬁlter is applied to a certain node, we only consider depth values within the zmax range from the depth of the node. However, with this procedure the bound- ary of the mesh can get tilted towards the camera as Fig. 7 shows. This happens because the symmetry of the ﬁlter is ∞ ∞ Figure 7: If certain depth values are not considered for ﬁl- tering, the mesh gets tilted towards the camera. Figure 8: Top: The standard approach produces bumpy sur- faces. Bottom: A ﬂat surface is generated by smoothing the depths in screen space. lost and the central value is pulled towards the values on the valid side of the ﬁlter. This problem can be solved easily. If the value at position i + k is omitted, both position i + k and position i − k are ignored in the weighted sum. When the depth of a node is changed, the depths of the silhouette nodes associated with it are changed by the same amount. 4.2. Silhouette Smoothing Smoothing of the depth values does not inﬂuence the appear- ance of the silhouettes of the mesh. To smooth the bound- ary of the mesh we smooth the screen space coordinates [xp, yp]T of the nodes of the mesh before the transforma- tion to world space. We use a very simple but effective iter- ative scheme. In each iteration, the screen space coordinates of each vertex are replaced by the average of its own co- ordinates and the coordinates of all adjacent vertices. The regular internal mesh resulting from case 0 in Fig. 5 and its ﬂipped conﬁguration is a ﬁx point of this smoothing proce- dure. Thus, it only affects the silhouettes as desired. To avoid the opening of internal silhouettes corresponding silhouette c⃝ Association for Computing Machinery, Inc. 2007. M. Müller et al. / Screen Space Meshes vertices are \"glued\" together in screen space during this pro- cess. The number of iterations niters is a user parameter. Sil- houette smoothing causes shrinking of outer silhouettes. In most cases this effect is desired. In order to cover the area of a puddle the ﬂuid particles have to have a certain size. When small groups of particles separate, the drops appear as rather large blobs. Silhouette smoothing makes the boundary of small sets of connected particles look more realistic (see Fig. 11). 5. Results All scenes were run on a Intel dual core CPU at 2.6GHz with 2GB of RAM. In most cases the SPH simulation was the bottleneck. The car wash scene (Fig. 1) contains 16K SPH particles. It runs at 20 fps without surface generation. The frame rate drops to 19 fps for screen spacing h = 6 and to 18 fps for screen spacing h = 3 with a triangle count of 13K and 40K respectively. The dungeon scene (Fig. 11) contains 10K particles. Its frame rate drops from 23 fps to 22 for h = 6 with 15K triangles and to 21 fps for h = 3 with 60K triangles. In both the slide scene (Fig. 9) and the wheels scene (Fig. 10) the particle count of 5K is small with respect to the number of triangles generated. Since 5K SPH particles can be simulated very efﬁciently – 65 fps in the slide scene – the time for the generation for the screen space mesh be- comes more signiﬁcant. For h = 6 the frame rate drops to 55 fps with 5 K triangles and to 40 fps for h = 3 to 40 fps with a triangle count of 20 K. 6. Conclusions and Future Work Screen space meshes provide an efﬁcient way to construct and visualize surfaces. We focused on the visualization of particle-based ﬂuids. However, the approach is more general and can be applied to other related visualization problems. The approach trades speed for certain limitations. The 3D mesh generated from the screen space mesh is only valid for the current camera position. This causes problems with shadow casting. A solution to this problem would be to gen- erate the ﬂuid mesh separately from the positions of the light sources. Also, only the front most surface layer is generated which does not produce visible artifacts in most scenarios as already mentioned. Most of the algorithm steps, such as the depth map gen- eration or the transformation of vertex positions, are well- suited to be computed on a GPU. We expect a GPU imple- mentation to be much faster than the CPU version we have, although the CPU version already allows the generation of quite complex surfaces in real-time as our results show. References [Bli82] BLINN J. F.: A generalization of algebraic surface drawing. ACM Trans. Graph. 1, 3 (1982), 235–256. [EMF02] ENRIGHT D., MARSCHNER S., FEDKIW R.: Animation and rendering of complex water surfaces. In Proceedings of the 29th annual conference on Computer graphics and interactive techniques (2002), ACM Press, pp. 736–744. [FF01] FOSTER N., FEDKIW R.: Practical animation of liquids. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques (2001), ACM Press, pp. 23–30. [GM77] GINGOLD R. A., MONAGHAN J. J.: Smoothed particle hydrodynamics: theory and application to non- spherical stars. Monthly Notices of the Royal Astronomi- cal Society (1977). [HNC02] HINSINGER D., NEYRET F., CANI M.-P.: In- teractive animation of ocean waves. In Proceedings of the ACM SIGGRAPH symposium on Computer animation (2002), ACM Press, pp. 161–166. [Joh04] JOHANSON C.: Real-time water rendering - in- troducing the projected grid concept. Master of Science Thesis (Lund University) (2004). [LC87] LORENSEN W. E., CLINE H. E.: Marching cubes: A high resolution 3d surface construction algorithm. In Proceedings of the 14th annual conference on Computer graphics and interactive techniques (1987), ACM Press, pp. 163–169. [MCG03] MÜLLER M., CHARYPAR D., GROSS M.: Particle-based ﬂuid simulation for interactive applica- tions. Proceedings of 2003 ACM SIGGRAPH Symposium on Computer Animation (2003), 154–159. [PTB∗03] PREMOZE S., TASDIZEN T., BIGLER J., LEFOHN A., WHITAKER R. T.: Particle-based simula- tion of ﬂuids. Eurographics 22, 3 (2003), 401–410. [Whi80] WHITTED T.: An improved illumination model for shaded display. Commun. ACM 23, 6 (1980), 343– 349. [ZPvBG01] ZWICKER M., PFISTER H., VAN BAAR J., GROSS M.: Surface splatting. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques (2001), ACM Press, pp. 371–378. c⃝ Association for Computing Machinery, Inc. 2007. M. Müller et al. / Screen Space Meshes Figure 9: Left: The original set of particles (their SPH density is color coded). Middle: Screen space mesh. Right: Final rendering. Figure 10: Left: A scene with a complex 3D liquid boundary. Middle: The screen space mesh with internal silhouettes. Left: A rotation of the frozen mesh reveals that the internal silhouettes are disconnected in the viewing direction as desired. Figure 11: Left: Flooding a corridor. Middle: Top view of the screen space mesh without silhouette smoothing. Right: Silhouette smoothing generates a surface tension effect. c⃝ Association for Computing Machinery, Inc. 2007.","libVersion":"0.2.2","langs":""}